# Exercise 10 - Question time!

## 1 - Why do we concatenate a column of ones to the left of the x vector when we use the linear algebra trick?

To simply "add" theta0 as bias instead of weight (theta0*1 + theta1+feature1 + theta2*feature2 +...)

## 2 - Why does the cost function square the distances between the data points and their predicted values?

To penalize harder big errors

## 3 - What does the cost functionâ€™s output represent?

how far off is our prediction

## 4 - Toward which value do we want the cost function to tend? What would that mean?

0.0 would mean that our model perfectly fit the truth values

## 5 - Do you understand why are matrix multiplications are not commutative?

yep
